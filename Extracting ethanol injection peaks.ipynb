{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55111b77-dd15-4610-a4e4-d48ae324faa9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Script #1\n",
    "This notebook was developed to generate individual csv files for the flat top ethanol injection peaks measured by tag <code>FI6320.PV</code> in <code>akbm-houston-prod.houston_data.sensor_data_scada</code>. The thought behind this script was to use an algorithm to identify individual peaks, and them extract them.\n",
    "\n",
    "To use this notebook successfully, the user will first have to identify regions where these nice regular peaks exist to identify a start time and and end time for the data series to be treated.\n",
    "\n",
    "The goal is further to integrate these ethanol injection peaks using different numerical methods to confirm that amount of ethanol corresponds with the expected amount per extraction recipe for krill oil manufacturing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc072ca-0cdb-4765-98d8-5db54a4a4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# ======= Connect to Google BigQuery using ODBC =======\n",
    "dsn = 'bq64_system'  # Your DSN\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)  # Ensure autocommit is enabled\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ======= User-defined global timestamps =======\n",
    "global_start_time = \"2025-01-18 00:00:01\"  # Change this as needed\n",
    "global_end_time = \"2025-01-18 04:38:22\"  # Change this as needed\n",
    "tagname = \"FI6320.PV\"  # Change this if needed\n",
    "\n",
    "# ======= Query to retrieve data from BigQuery =======\n",
    "query = f\"\"\"\n",
    "SELECT DISTINCT\n",
    "  time AS timestamp,\n",
    "  value AS value\n",
    "FROM \n",
    "  `akbm-houston-prod.houston_data.sensor_data_scada`\n",
    "WHERE \n",
    "  time BETWEEN TIMESTAMP('{global_start_time}') AND TIMESTAMP('{global_end_time}')\n",
    "  AND time >= '1900-01-01' -- Partition filter for performance\n",
    "  AND tagname = '{tagname}'\n",
    "ORDER BY \n",
    "  timestamp;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch all results and convert from pyodbc.Row to a list of tuples\n",
    "data = [tuple(row) for row in cursor.fetchall()]  # Ensure each row is a normal tuple\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[col[0] for col in cursor.description])\n",
    "\n",
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Convert 'timestamp' column to datetime for proper time-based operations\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# ======= Detect flat-top peaks =======\n",
    "peak_threshold = 9000  # Adjust threshold as needed\n",
    "df['is_peak'] = df['value'] > peak_threshold\n",
    "\n",
    "# Group peaks based on continuous regions\n",
    "df['peak_group'] = (df['is_peak'] != df['is_peak'].shift()).cumsum()\n",
    "peak_groups = df[df['is_peak']].groupby('peak_group')\n",
    "\n",
    "# Store detected peaks\n",
    "refined_peak_segments = [group for _, group in peak_groups if len(group) > 5]\n",
    "\n",
    "# ======= Identify baseline midpoints between peaks =======\n",
    "baseline_midpoints = []\n",
    "for i in range(len(refined_peak_segments) - 1):\n",
    "    peak_end = refined_peak_segments[i]['timestamp'].max()\n",
    "    next_peak_start = refined_peak_segments[i + 1]['timestamp'].min()\n",
    "    midpoint = peak_end + (next_peak_start - peak_end) / 2  # Midpoint as separation\n",
    "    baseline_midpoints.append(midpoint)\n",
    "\n",
    "# ======= Define segment boundaries =======\n",
    "global_start_time = df['timestamp'].min()\n",
    "global_end_time = df['timestamp'].max()\n",
    "segmentation_boundaries = [global_start_time] + baseline_midpoints + [global_end_time]\n",
    "\n",
    "# Adjust segment starts so each starts at the **previous baseline midpoint** (not peak end)\n",
    "segment_starts = [global_start_time] + baseline_midpoints[:-1]  # Start at previous baseline\n",
    "segment_ends = baseline_midpoints + [global_end_time]  # End at next baseline\n",
    "\n",
    "# ======= Extract individual segments and save to CSV =======\n",
    "csv_files = []\n",
    "for i, (start, end) in enumerate(zip(segment_starts, segment_ends)):\n",
    "    segment_df = df[(df['timestamp'] >= start) & (df['timestamp'] <= end)]\n",
    "    \n",
    "    # Save segment to CSV\n",
    "    file_name = f\"segment_{i+1}.csv\"\n",
    "    segment_df.to_csv(file_name, index=False)\n",
    "    csv_files.append(file_name)\n",
    "    print(f\"Saved: {file_name} ({len(segment_df)} rows)\")\n",
    "\n",
    "# ======= Extract final portion of data after last detected baseline =======\n",
    "if baseline_midpoints:\n",
    "    last_cutoff = baseline_midpoints[-1]  # Use the last detected midpoint\n",
    "else:\n",
    "    last_cutoff = segment_ends[-1]  # Fallback to the last segment end if no midpoints\n",
    "\n",
    "final_segment_df = df[df['timestamp'] > last_cutoff]  # Ensure all remaining data is included\n",
    "final_file_name = \"segment_final.csv\"\n",
    "final_segment_df.to_csv(final_file_name, index=False)\n",
    "csv_files.append(final_file_name)\n",
    "print(f\"Saved: {final_file_name} ({len(final_segment_df)} rows)\")\n",
    "\n",
    "print(\"\\nAll segments successfully saved as CSV files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b1c3f9-b397-47e3-a92b-40e713a7e178",
   "metadata": {},
   "source": [
    "### Script #2 (currently the preferred solution. ignore script #1 for now)\n",
    "##### Getting individual csv files for each extraction tank filling\n",
    "This script was developed to generate individual csv files for individual ethanol injection peaks measured by tag <code>FI6320.PV</code> in <code>akbm-houston-prod.houston_data.sensor_data_scada</code>.<br>\n",
    "\n",
    "To execute this script, the user provides an Excel list containing timestamps for the start and end times of each peak. The start and end times can, for example, be determined manually.\n",
    "\n",
    "> v1: uses timestamps provided in Excel file, loops through them and gets the individual peaks <br>\n",
    "> v2: performs baseline correction & adds a column showing time in seconds from start to end of each signal extracted.<br>\n",
    "> The name of each csv file is the first time stamp in each csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d5ed1-3ea9-4dfc-8605-7d9e3048c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1: no baseline correction and no \"time passed\" column showing time from 0 to end in seconds for each peak\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ======= Connect to Google BigQuery using ODBC =======\n",
    "dsn = 'bq64_system'  # Your DSN\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)  # Ensure autocommit is enabled\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ======= Read start and end times from Excel =======\n",
    "# Assumes that the Excel file \"start & end times.xlsx\" has no header and that:\n",
    "#   Column 0 = start time, Column 1 = end time.\n",
    "# If your Excel file has headers, adjust accordingly.\n",
    "# timestamps are in format \"15.05.2025  00:05:36\" in each cell. This is UTC time as dd.mm.yyyy hh:mm:ss\n",
    "time_ranges = pd.read_excel(\"timestamps for FI6320.xlsx\", header=None)\n",
    "\n",
    "tagname = \"FI6320.PV\"\n",
    "\n",
    "# ======= Loop through each time range row =======\n",
    "for idx, row in time_ranges.iterrows():\n",
    "    # Terminate loop if both timestamp cells are missing (i.e. we've reached the end)\n",
    "    if pd.isna(row[0]) and pd.isna(row[1]):\n",
    "        print(f\"No timestamp found in row {idx}. Terminating loop.\")\n",
    "        break\n",
    "\n",
    "    # Convert Excel values to datetime objects; errors will raise if conversion fails\n",
    "    start_time = pd.to_datetime(row[0], errors='raise')\n",
    "    end_time = pd.to_datetime(row[1], errors='raise')\n",
    "    \n",
    "    # Format timestamps for the BigQuery query (YYYY-MM-DD HH:MM:SS format)\n",
    "    start_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Build the query using the current time range\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "      time AS timestamp,\n",
    "      value AS value\n",
    "    FROM \n",
    "      `akbm-houston-prod.houston_data.sensor_data_scada`\n",
    "    WHERE \n",
    "      time BETWEEN TIMESTAMP('{start_str}') AND TIMESTAMP('{end_str}')\n",
    "      AND time >= '1900-01-01' -- Partition filter for performance\n",
    "      AND tagname = '{tagname}'\n",
    "    ORDER BY \n",
    "      timestamp;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "    data = [tuple(r) for r in cursor.fetchall()]  # Convert each row to a normal tuple\n",
    "    \n",
    "    # Convert fetched data to a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[desc[0] for desc in cursor.description])\n",
    "    \n",
    "    # Use the start time in 'dd-mm-yyyy-hh-mm' format for the CSV filename\n",
    "    file_timestamp = start_time.strftime(\"%d-%m-%Y-%H-%M\")\n",
    "    csv_filename = f\"data_series_{file_timestamp}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Exported data series starting at {file_timestamp} to {csv_filename}\")\n",
    "\n",
    "# ======= Clean up =======\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f9f31-d6c1-4158-a3e9-bad2ce260aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2: performs baseline correction & adds a column showing time in seconds from start to end of each signal extracted.\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# ======= Connect to Google BigQuery using ODBC =======\n",
    "dsn = 'bq64_system'  # Your DSN\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)  # Ensure autocommit is enabled\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# ======= Read start and end times from Excel =======\n",
    "# what the Excel file should look like\n",
    "#      no headers\n",
    "#      start time in column A (format: dd.mm.yyyy hh:mm:ss, example:11.05.2025  23:53:40)\n",
    "#      end time in column B, same format as start time\n",
    "time_ranges = pd.read_excel(\"timestamps for FI6320.xlsx\", header=None)\n",
    "\n",
    "tagname = \"FI6320.PV\"\n",
    "\n",
    "# ======= Loop through each time range row =======\n",
    "for idx, row in time_ranges.iterrows():\n",
    "    # Terminate loop if both timestamp cells are missing (i.e. we've reached the end)\n",
    "    if pd.isna(row[0]) and pd.isna(row[1]):\n",
    "        print(f\"No timestamp found in row {idx}. Terminating loop.\")\n",
    "        break\n",
    "\n",
    "    # Convert Excel values to datetime objects; errors will raise if conversion fails\n",
    "    start_time = pd.to_datetime(row[0], errors='raise')\n",
    "    end_time = pd.to_datetime(row[1], errors='raise')\n",
    "    \n",
    "    # Format timestamps for the BigQuery query (YYYY-MM-DD HH:MM:SS format)\n",
    "    start_str = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    end_str = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Build the query using the current time range\n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "      time AS timestamp,\n",
    "      value AS value\n",
    "    FROM \n",
    "      `akbm-houston-prod.houston_data.sensor_data_scada`\n",
    "    WHERE \n",
    "      time BETWEEN TIMESTAMP('{start_str}') AND TIMESTAMP('{end_str}')\n",
    "      AND time >= '1900-01-01' -- Partition filter for performance\n",
    "      AND tagname = '{tagname}'\n",
    "    ORDER BY \n",
    "      timestamp;\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "    data = [tuple(r) for r in cursor.fetchall()]  # Convert each row to a normal tuple\n",
    "    \n",
    "    # Convert fetched data to a DataFrame\n",
    "    df = pd.DataFrame(data, columns=[desc[0] for desc in cursor.description])\n",
    "    \n",
    "    # Skip CSV export if no data is returned\n",
    "    if df.empty:\n",
    "        print(f\"No data returned for time range starting {start_str}. Skipping CSV export.\")\n",
    "        continue\n",
    "    \n",
    "    # Modification 1: Baseline correction\n",
    "    # Create a new column that subtracts 1.71578 from \"value\"\n",
    "    df['value_corrected'] = df['value'] - 1.71578\n",
    "    \n",
    "    # Modification 2: Add a column for cumulative seconds passed\n",
    "    # Convert the \"timestamp\" column to datetime\n",
    "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
    "    # Compute the time difference (in seconds) between consecutive timestamps and perform a cumulative sum\n",
    "    df['seconds_passed'] = df['timestamp_dt'].diff().dt.total_seconds().fillna(0).cumsum()\n",
    "    # Drop the intermediate datetime column if not needed\n",
    "    df.drop(columns=['timestamp_dt'], inplace=True)\n",
    "    \n",
    "    # Use the start time in 'dd-mm-yyyy-hh-mm' format for the CSV filename\n",
    "    file_timestamp = start_time.strftime(\"%d-%m-%Y-%H-%M\")\n",
    "    csv_filename = f\"data_series_{file_timestamp}.csv\"\n",
    "    \n",
    "    # Export the DataFrame to a CSV file\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Exported data series starting at {file_timestamp} to {csv_filename}\")\n",
    "\n",
    "# ======= Clean up =======\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf1805c-7437-4726-be89-0210bb3b2041",
   "metadata": {},
   "source": [
    "### Script 3\n",
    "#### Signal integration & aggregation\n",
    "Integrating aggregating the area under the curve for each csv file.<br>\n",
    "\n",
    "* <b>Trapezoidal rule & Rieman are \"time aware\" integration methods, giving consideration to the fact that timepoints are not evenly spaced.</b><br>\n",
    "* Simpson's rule is more sensitive to uneven time grids, but the results between the three methods are all in relatively good agreement.<br>\n",
    "* Aggregation (simple summation) is sensitive to the spacing of the time steps. **Resampling with interpolation between missing data to fill the gaps can be used to compensate**.<br>\n",
    "\n",
    "<u>The follwing results are given by the script below in the exported csv:\n",
    "| Metric Name                                      | Explanation |\n",
    "|--------------------------------------------------|-------------|\n",
    "| **Original data results:**                       |             |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_trapezoid_value       | from original data, without resampling, no baseline correction             |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_simpson_value         | from original data, without resampling, no baseline correction           |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_riemann_value         | from original data, without resampling, no baseline correction            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_trapezoid_value_corr  | from original data, without resampling, baseline corrected            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_simpson_value_corr    | from original data, without resampling, baseline corrected            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_riemann_value_corr    | from original data, without resampling, baseline corrected            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_sum_value             | aggregation, from original data, without resampling, no baseline correction            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;orig_sum_value_corr        | aggregation, from original data, without resampling, baseline corrected            |\n",
    "| **Resampled data results:**                      |             |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_trapezoid_value        | from resampled data, no baseline correction           |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_simpson_value          | from resampled data, no baseline correction            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_riemann_value          | from resampled data, no baseline correction            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_trapezoid_value_corr     | from resampled data, baseline corrected             |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_simpson_value_corr       | from resampled data, baseline corrected            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_riemann_value_corr       | from resampled data, baseline corrected            |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_sum_value              | aggergation, from resampled data, no baseline correction             |\n",
    "| &nbsp;&nbsp;&nbsp;&nbsp;res_sum_value_corr         | aggergation, from resampled data, baseline corrected           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d3455-47cf-45f0-807c-2f6c372bdb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy.integrate import simpson  # Make sure you have scipy installed\n",
    "\n",
    "# Set the directory where your CSV files are located\n",
    "directory_path = \"peaks\"  # e.g., \"data\" or \"./\"\n",
    "\n",
    "# Pattern to match your CSV files\n",
    "csv_files = glob.glob(f\"{directory_path}/data_series_*.csv\")\n",
    "\n",
    "# List to store summary results\n",
    "summary_results = []\n",
    "\n",
    "for file in csv_files:\n",
    "    # ---------------------------\n",
    "    # ORIGINAL DATA CALCULATIONS\n",
    "    # ---------------------------\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # Convert \"timestamp\" column to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # If \"seconds_passed\" is not in the DataFrame, compute it\n",
    "    if \"seconds_passed\" not in df.columns:\n",
    "        df['seconds_passed'] = df['timestamp'].diff().dt.total_seconds().fillna(0).cumsum()\n",
    "    \n",
    "    # Prepare arrays for original data integration\n",
    "    x_orig = df['seconds_passed'].values\n",
    "    y_value_orig = df['value'].values\n",
    "    y_value_corr_orig = df['value_corrected'].values\n",
    "    \n",
    "    # Integration on original data:\n",
    "    # 1. Trapezoidal rule\n",
    "    orig_trap_value = np.trapezoid(y_value_orig, x_orig) / 3600\n",
    "    orig_trap_value_corr = np.trapezoid(y_value_corr_orig, x_orig) / 3600\n",
    "    \n",
    "    # 2. Simpson's rule\n",
    "    try:\n",
    "        orig_simpson_value = simpson(y_value_orig, x_orig) / 3600\n",
    "    except Exception as e:\n",
    "        orig_simpson_value = np.nan\n",
    "        print(f\"Simpson integration error for {file} on original 'value': {e}\")\n",
    "    try:\n",
    "        orig_simpson_value_corr = simpson(y_value_corr_orig, x_orig) / 3600\n",
    "    except Exception as e:\n",
    "        orig_simpson_value_corr = np.nan\n",
    "        print(f\"Simpson integration error for {file} on original 'value_corrected': {e}\")\n",
    "    \n",
    "    # 3. Riemann sum (left endpoint)\n",
    "    orig_riemann_value = np.sum(y_value_orig[:-1] * np.diff(x_orig)) / 3600\n",
    "    orig_riemann_value_corr = np.sum(y_value_corr_orig[:-1] * np.diff(x_orig)) / 3600\n",
    "    \n",
    "    # Aggregation (simple sum) for original data\n",
    "    orig_sum_value = np.sum(y_value_orig) / 3600\n",
    "    orig_sum_value_corr = np.sum(y_value_corr_orig) / 3600\n",
    "    \n",
    "    # ----------------------------\n",
    "    # RESAMPLED DATA CALCULATIONS\n",
    "    # ----------------------------\n",
    "    # Create a copy and set timestamp as index\n",
    "    df_resample = df.copy()\n",
    "    df_resample.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    # Resample the data to a fixed 1-second frequency and interpolate missing values\n",
    "    resampled_df = df_resample.resample('1s').mean().interpolate('linear')\n",
    "    \n",
    "    # Create a \"seconds_passed\" column based on the resampled index\n",
    "    resampled_df['seconds_passed'] = (resampled_df.index - resampled_df.index[0]).total_seconds()\n",
    "    \n",
    "    # Prepare arrays for resampled data integration\n",
    "    x_res = resampled_df['seconds_passed'].values\n",
    "    y_value_res = resampled_df['value'].values\n",
    "    y_value_corr_res = resampled_df['value_corrected'].values\n",
    "    \n",
    "    # Integration on resampled data:\n",
    "    # 1. Trapezoidal rule\n",
    "    res_trap_value = np.trapezoid(y_value_res, x_res) / 3600\n",
    "    res_trap_value_corr = np.trapezoid(y_value_corr_res, x_res) / 3600\n",
    "    \n",
    "    # 2. Simpson's rule\n",
    "    try:\n",
    "        res_simpson_value = simpson(y_value_res, x_res) / 3600\n",
    "    except Exception as e:\n",
    "        res_simpson_value = np.nan\n",
    "        print(f\"Simpson integration error for {file} on resampled 'value': {e}\")\n",
    "    try:\n",
    "        res_simpson_value_corr = simpson(y_value_corr_res, x_res) / 3600\n",
    "    except Exception as e:\n",
    "        res_simpson_value_corr = np.nan\n",
    "        print(f\"Simpson integration error for {file} on resampled 'value_corrected': {e}\")\n",
    "    \n",
    "    # 3. Riemann sum (left endpoint)\n",
    "    res_riemann_value = np.sum(y_value_res[:-1] * np.diff(x_res)) / 3600\n",
    "    res_riemann_value_corr = np.sum(y_value_corr_res[:-1] * np.diff(x_res)) / 3600\n",
    "    \n",
    "    # Aggregation (simple sum) for resampled data\n",
    "    res_sum_value = np.sum(y_value_res) / 3600\n",
    "    res_sum_value_corr = np.sum(y_value_corr_res) / 3600\n",
    "    \n",
    "    # Get the first timestamp (for reference)\n",
    "    first_timestamp = df['timestamp'].iloc[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Append results for both original and resampled data to the summary list\n",
    "    summary_results.append({\n",
    "        'filename': file,\n",
    "        'first_timestamp': first_timestamp,\n",
    "        # Original data results:\n",
    "        'orig_trapezoid_value': orig_trap_value,\n",
    "        'orig_simpson_value': orig_simpson_value,\n",
    "        'orig_riemann_value': orig_riemann_value,\n",
    "        'orig_trapezoid_value_corr': orig_trap_value_corr,\n",
    "        'orig_simpson_value_corr': orig_simpson_value_corr,\n",
    "        'orig_riemann_value_corr': orig_riemann_value_corr,\n",
    "        'orig_sum_value': orig_sum_value,\n",
    "        'orig_sum_value_corr': orig_sum_value_corr,\n",
    "        # Resampled data results:\n",
    "        'res_trapezoid_value': res_trap_value,\n",
    "        'res_simpson_value': res_simpson_value,\n",
    "        'res_riemann_value': res_riemann_value,\n",
    "        'res_trapezoid_value_corr': res_trap_value_corr,\n",
    "        'res_simpson_value_corr': res_simpson_value_corr,\n",
    "        'res_riemann_value_corr': res_riemann_value_corr,\n",
    "        'res_sum_value': res_sum_value,\n",
    "        'res_sum_value_corr': res_sum_value_corr,\n",
    "    })\n",
    "\n",
    "# Create a summary DataFrame and print results (no new CSV files are created)\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "print(summary_df)\n",
    "\n",
    "# Save the summary DataFrame to a CSV file\n",
    "summary_df.to_csv(\"integration_summary.csv\", index=False)\n",
    "print(\"Integration summary saved to integration_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073e148b-69fe-41ea-af68-1b0bfa6dd7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
