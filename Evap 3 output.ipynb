{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4173b886-2723-423f-80d1-36e0ead808b0",
   "metadata": {},
   "source": [
    "This script is developed to get sensor data for the finished product flow out of Evap 3, and transform this information, for example by integration.\n",
    "\n",
    "The output from Evap 3 is finished product and there are a variety of potentially useful sensors that could aid investigations targeted at correlating extraction process changes with finished product changes.\n",
    "\n",
    "It is easy enough to get the raw data series from the data warehouse, but since we want to integrate some of sensor signals (like mass flow in kg/hr into a cumulative mass), we're well advised to make use of Python's rich math libraries, rather than trying some approximate solution with JMP.\n",
    "\n",
    "What do we want from this script:\n",
    "* Pull individual data series from GCP as 1-minute averages\n",
    "    * Mass flow (kg/hr) out of Evap 3: <code>tagname = 'FI323113_EvaporatorThreeDischarge' </code> in <code>akbm-houston-prod.houston_data.sensor_data_scada</code>\n",
    "    * Mass flow (lb/hr) out of Evap 3: <code>tagname = FIT_323113</code> in <code>akbm-houston-prod.houston_data.sensor_data_sulzer2</code>\n",
    "    * Phospholipid concentration in mass flow out of Evap 3: <code>tagname = Evaporator 3 Discharge (AE3800A).Phospholipids.Value</code> in <code>akbm-houston-prod.houston_data.sensor_data_ftnir</code>\n",
    "* Add two new columns showing the mass flow of total PL from Evap 3. This is the product between mass flow and PL percentage (formula: mass flow/100 * PL)\n",
    "* Integrate (as in \"calculate the area under the curve) the signals over defined time intervals (e.g. integrate output mass from Evap 3 for every 1h or every shift, and so on). Variables for pulling data:\n",
    "    * Start time (06:00 AM & 06:00 PM are local CDT start times for day & night shift in Houston respectively)\n",
    "    * Interval for integration (e.g. 12 hours to get integrated results for day & night shifts)\n",
    "    * Granularity (SQL: SECOND, MINUTE, HOUR ...). However, 1-minute granularity is possibly not an issue considering the size of the dataset.\n",
    "* Give a summary table for the integration results.\n",
    "    * Use different integration methods for cross checking:\n",
    "        * Trapezoidal rule\n",
    "        * Simpson's rule\n",
    "        * Rieman sum\n",
    "        * Aggregation (OK, this is not integration, but we want it for comparison)\n",
    " * Give output as csv with clearly labelled column headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef556e06-a98b-40d6-8c97-ce9e81de42a0",
   "metadata": {},
   "source": [
    "The below script gives the following results of the specified intervals (INTERVAL_HOURS), starting with with START_TIME:\n",
    "* Total mass in kg\n",
    "* Total mass in lb\n",
    "* PL mass in kg\n",
    "* PL mass in lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fc483-b66a-462c-8df9-023f75067e89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28caea37-10d6-452a-9f2e-b82412df12cf",
   "metadata": {},
   "source": [
    "**IMPORTANT(?)**<br>\n",
    "When testing the below script and comparing it to a manual point & click - based procedure using JMP, deviations were observed for the aggregated data. \n",
    "\n",
    "To reproduce this test use the following parameters:<br>\n",
    "* <code>START_TIME      = datetime(2025, 5, 10, 0, 0, 0)</code><br>\n",
    "* <code>END_TIME        = datetime(2025, 6, 2, 0, 0, 0)<code>\n",
    "* <code>INTERVAL_HOURS  = 12</code>\n",
    "* use 1-minute granularity\n",
    "* use the JSL code block shown after the Python code block right below to get JMP-based dataset\n",
    "\n",
    "On 02.Jun.'25 at around 16:30 local time in Oslo, the joined data sets containing aggregated data had 46 rows. Comparing the 12h aggregates from point & click via JMP to the Python script below, the differences for 11 of the rows were much bigger than zero. However, the relative difference ranged only from approx. 0,1-0,4% rel., which is why this isn't a high priority issue for now. These are the timestamps for the rows where deviations were found:\n",
    "\n",
    "11.05.2025 00:00<br>\n",
    "12.05.2025 12:00<br>\n",
    "23.05.2025 00:00<br>\n",
    "24.05.2025 12:00<br>\n",
    "25.05.2025 00:00<br>\n",
    "27.05.2025 12:00<br>\n",
    "28.05.2025 12:00<br>\n",
    "30.05.2025 00:00<br>\n",
    "30.05.2025 12:00<br>\n",
    "31.05.2025 00:00<br>\n",
    "01.06.2025 12:00<br>\n",
    "\n",
    "Each timestamp is the start of a 12-h interval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a5f12f-1f49-4929-bf97-1e21ecaae712",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330e4a0-36d5-48ac-a980-7d898bf63d31",
   "metadata": {},
   "source": [
    "### The below code is based on 1-minute averages of the tag data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de12dc-c9ed-45d5-a5fb-6d99b96a9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.integrate import simpson\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ─── USER‐DEFINED PARAMETERS ────────────────────────────────────────────────────\n",
    "# 1) Time window for data pull (UTC)\n",
    "START_TIME      = datetime(2025, 5, 10, 11, 0, 0)   # SQL‐query start AND first integration interval start, (yyyy, m, d, hr, min, sec)\n",
    "END_TIME        = datetime(2025, 6, 23, 23, 0, 0)   # SQL‐query end (and integration cutoff)\n",
    "# remember, time in data warehouse is UTC time\n",
    "\n",
    "# 2) Integration interval length (hours)\n",
    "INTERVAL_HOURS  = 12                          \n",
    "\n",
    "# 3) BigQuery table & tag names\n",
    "SCADA_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_scada\"\n",
    "TAG_MF_KGHR     = \"FI323113_EvaporatorThreeDischarge\"\n",
    "\n",
    "SULZER_TABLE    = \"akbm-houston-prod.houston_data.sensor_data_sulzer2\"\n",
    "TAG_MF_LBHR     = \"FIT_323113\"\n",
    "\n",
    "FTNIR_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_ftnir\"\n",
    "TAG_PL_PCT      = \"Evaporator 3 Discharge (AE3800A).Phospholipids.Value\"\n",
    "\n",
    "# 4) CSV output filename\n",
    "OUT_CSV         = \"evap3_integration_summary.csv\"\n",
    "\n",
    "# ─── 1) CONNECT TO BigQuery via ODBC ──────────────────────────────────────────\n",
    "dsn = 'bq64_system'\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)\n",
    "\n",
    "# ─── 2) FUNCTION TO PULL A 1-MINUTE‐AVERAGED SERIES ───────────────────────────\n",
    "def fetch_1min_series(\n",
    "    conn: pyodbc.Connection,\n",
    "    dataset_table: str,\n",
    "    tagname: str,\n",
    "    start_time: datetime,\n",
    "    end_time: datetime\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Queries BigQuery to pull a single tag as a 1-minute‐averaged time series\n",
    "    between start_time and end_time (inclusive). Returns a DataFrame with:\n",
    "        ┌───────────────────┬──────────┐\n",
    "        │     timestamp     │   value  │\n",
    "        ├───────────────────┼──────────┤\n",
    "        │ 2025-06-01 00:00  │ 123.456  │\n",
    "        │ 2025-06-01 00:01  │ 120.789  │\n",
    "        │   ...             │   ...    │\n",
    "        └───────────────────┴──────────┘\n",
    "    Assumes each table has columns: `timestamp` (TIMESTAMP), `value`, and `tagname`.\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    t1 = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "      TIMESTAMP_TRUNC(time, MINUTE) AS ts_min,\n",
    "      AVG(value) AS avg_value\n",
    "    FROM `{dataset_table}`\n",
    "    WHERE tagname = '{tagname}'\n",
    "      AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "    GROUP BY ts_min\n",
    "    ORDER BY ts_min\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df = df.rename(columns={\"ts_min\": \"timestamp\", \"avg_value\": \"value\"})\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\").sort_index()\n",
    "    return df\n",
    "\n",
    "# ─── 3) PULL THE THREE SERIES AND MERGE THEM ─────────────────────────────────\n",
    "df_kg  = fetch_1min_series(conn, SCADA_TABLE, TAG_MF_KGHR, START_TIME, END_TIME)\n",
    "df_lb  = fetch_1min_series(conn, SULZER_TABLE, TAG_MF_LBHR, START_TIME, END_TIME)\n",
    "df_plp = fetch_1min_series(conn, FTNIR_TABLE, TAG_PL_PCT,  START_TIME, END_TIME)\n",
    "\n",
    "df_kg  = df_kg.rename(columns={\"value\": \"mass_flow_kg_hr\"})\n",
    "df_lb  = df_lb.rename(columns={\"value\": \"mass_flow_lb_hr\"})\n",
    "df_plp = df_plp.rename(columns={\"value\": \"pl_pct\"})\n",
    "\n",
    "df_all = df_kg.join(df_lb, how=\"inner\").join(df_plp, how=\"inner\")\n",
    "# Now df_all has columns: mass_flow_kg_hr, mass_flow_lb_hr, pl_pct\n",
    "\n",
    "# ─── 4) COMPUTE “TOTAL PL MASS” COLUMNS (kg/hr & lb/hr) ──────────────────────\n",
    "df_all[\"pl_mass_kg_hr\"] = df_all[\"mass_flow_kg_hr\"] * (df_all[\"pl_pct\"] / 100.0)\n",
    "df_all[\"pl_mass_lb_hr\"] = df_all[\"mass_flow_lb_hr\"] * (df_all[\"pl_pct\"] / 100.0)\n",
    "# Now df_all columns: mass_flow_kg_hr, mass_flow_lb_hr, pl_pct, pl_mass_kg_hr, pl_mass_lb_hr\n",
    "\n",
    "# ─── 5) BUILD INTEGRATION INTERVALS FROM SINGLE START + DURATION ──────────────\n",
    "def build_intervals_from_start(\n",
    "    start: datetime,\n",
    "    end: datetime,\n",
    "    interval_hours: int\n",
    ") -> List[Tuple[datetime, datetime]]:\n",
    "    \"\"\"\n",
    "    Splits [start, end) into successive chunks of length `interval_hours`.\n",
    "    Each interval_i is [start + i*interval_hours, start + (i+1)*interval_hours),\n",
    "    with the final interval clipped at `end`.\n",
    "    \"\"\"\n",
    "    intervals: List[Tuple[datetime, datetime]] = []\n",
    "    cursor = start\n",
    "    delta = timedelta(hours=interval_hours)\n",
    "\n",
    "    while cursor < end:\n",
    "        next_point = cursor + delta\n",
    "        s = cursor\n",
    "        e = min(next_point, end)\n",
    "        intervals.append((s, e))\n",
    "        cursor = next_point\n",
    "\n",
    "    return intervals\n",
    "\n",
    "interval_list = build_intervals_from_start(START_TIME, END_TIME, INTERVAL_HOURS)\n",
    "\n",
    "# ─── 6) FUNCTION TO COMPUTE INTEGRALS PER INTERVAL ────────────────────────────\n",
    "def integrate_interval(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    start: datetime,\n",
    "    end: datetime\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Given a DataFrame `df` indexed by timestamp (1-minute frequency),\n",
    "    and a column name `col` (e.g. 'mass_flow_kg_hr'),\n",
    "    plus an interval [start, end), compute:\n",
    "      1) trapezoidal rule\n",
    "      2) Simpson’s rule\n",
    "      3) Left‐Riemann sum\n",
    "      4) aggregation = sum of values\n",
    "\n",
    "    All flows are in “per hour” units (e.g. kg/hr). Integrating over hours\n",
    "    yields “kg” (or “lb” if original was lb/hr).\n",
    "    Returns (trapz, simpson, riemann, agg_sum).\n",
    "    \"\"\"\n",
    "    df_int = df.loc[start : end - timedelta(minutes=1), col].dropna()\n",
    "    if df_int.empty:\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    # Convert timestamps to hours since epoch\n",
    "    t_secs = (df_int.index.view(np.int64) // 10**9).astype(float)\n",
    "    t_hours = t_secs / 3600.0\n",
    "    y = df_int.values\n",
    "\n",
    "    # A) Trapezoidal\n",
    "    trapz_val = np.trapz(y, x=t_hours)\n",
    "\n",
    "    # B) Simpson’s rule (needs ≥3 points; drop last if even count)\n",
    "    if len(y) >= 3:\n",
    "        if len(y) % 2 == 0:\n",
    "            t_sim = t_hours[:-1]\n",
    "            y_sim = y[:-1]\n",
    "        else:\n",
    "            t_sim = t_hours\n",
    "            y_sim = y\n",
    "        simpson_val = simpson(y_sim, x=t_sim)\n",
    "    else:\n",
    "        simpson_val = np.nan\n",
    "\n",
    "    # C) Left‐Riemann sum: sum(y[i] * (t[i+1] − t[i])) for i=0..N-2\n",
    "    if len(y) >= 2:\n",
    "        riemann_val = np.sum(y[:-1] * (t_hours[1:] - t_hours[:-1]))\n",
    "    else:\n",
    "        riemann_val = 0.0\n",
    "\n",
    "    # D) Aggregation (sum of raw values)\n",
    "    agg_sum_val = np.sum(y) * (1.0 / 60.0)\n",
    "\n",
    "    return (trapz_val, simpson_val, riemann_val, agg_sum_val)\n",
    "\n",
    "# ─── 7) LOOP OVER INTERVALS AND BUILD SUMMARY TABLE ───────────────────────────\n",
    "records = []\n",
    "for (t0, t1) in interval_list:\n",
    "    trapz_kg, simps_kg, riem_kg, agg_kg       = integrate_interval(df_all, \"mass_flow_kg_hr\", t0, t1)\n",
    "    trapz_plkg, simps_plkg, riem_plkg, agg_plkg = integrate_interval(df_all, \"pl_mass_kg_hr\",   t0, t1)\n",
    "    trapz_lb, simps_lb, riem_lb, agg_lb       = integrate_interval(df_all, \"mass_flow_lb_hr\", t0, t1)\n",
    "    trapz_pllb, simps_pllb, riem_pllb, agg_pllb = integrate_interval(df_all, \"pl_mass_lb_hr\",   t0, t1)\n",
    "\n",
    "    records.append({\n",
    "        \"interval_start\":    t0,\n",
    "        \"interval_end\":      t1,\n",
    "        # mass_flow_kg results (kg)\n",
    "        \"trapz_mass_kg\":     trapz_kg,\n",
    "        \"simpson_mass_kg\":   simps_kg,\n",
    "        \"riemann_mass_kg\":   riem_kg,\n",
    "        \"agg_sum_mass_kg\":   agg_kg,\n",
    "        # pl_mass_kg results (kg)\n",
    "        \"trapz_pl_kg\":       trapz_plkg,\n",
    "        \"simpson_pl_kg\":     simps_plkg,\n",
    "        \"riemann_pl_kg\":     riem_plkg,\n",
    "        \"agg_sum_pl_kg\":     agg_plkg,\n",
    "        # mass_flow_lb results (lb)\n",
    "        \"trapz_mass_lb\":     trapz_lb,\n",
    "        \"simpson_mass_lb\":   simps_lb,\n",
    "        \"riemann_mass_lb\":   riem_lb,\n",
    "        \"agg_sum_mass_lb\":   agg_lb,\n",
    "        # pl_mass_lb results (lb)\n",
    "        \"trapz_pl_lb\":       trapz_pllb,\n",
    "        \"simpson_pl_lb\":     simps_pllb,\n",
    "        \"riemann_pl_lb\":     riem_pllb,\n",
    "        \"agg_sum_pl_lb\":     agg_pllb,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame.from_records(records)\n",
    "summary_df[\"interval_start\"] = pd.to_datetime(summary_df[\"interval_start\"])\n",
    "summary_df[\"interval_end\"]   = pd.to_datetime(summary_df[\"interval_end\"])\n",
    "summary_df = summary_df.sort_values(\"interval_start\").reset_index(drop=True)\n",
    "\n",
    "# ─── 8) WRITE SUMMARY TABLE TO CSV ────────────────────────────────────────────\n",
    "summary_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Wrote summary table with {len(summary_df)} intervals to → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1444b98-e856-45ea-89d5-88265bd1f561",
   "metadata": {},
   "source": [
    "//##################################################\n",
    "//JSL code block:\n",
    "//##################################################\n",
    "// run it, then do the aggregation manually \n",
    "// see this web link for how to aggregate by point & click: https://community.jmp.com/t5/Discussions/Sum-values/td-p/256629\n",
    "\n",
    "\n",
    "ODBC        = \"ODBC:DSN=bq64_system\";\n",
    "\n",
    "start       = \"'2025-05-10 00:00:00'\";  // yyyy-mm-dd HH:MM:SS\n",
    "stop        = \"'2025-06-30 23:59:59'\";\n",
    "\n",
    "granularity = \"MINUTE\";\n",
    "\n",
    "sql_template = \"\\[\n",
    "-- PL, EtOH, o-3 (wt%) NIR from FTNIR sensor\n",
    "\tSELECT\n",
    "\t\ttagname,\n",
    "\t\tTIMESTAMP_TRUNC(time, ¤ granularity ¤) AS time,\n",
    "\t\tAVG(value) AS avg_value\n",
    "\t\tFROM `akbm-houston-prod.houston_data.sensor_data_ftnir`\n",
    "\t\tWHERE tagname IN (\n",
    "\t\t\t'Evaporator 3 Discharge (AE3800A).Phospholipids.Value',\n",
    "\t\t\t'Evaporator 3 Discharge (AE3800A).Ethanol.Value',\n",
    "\t\t\t'Evaporator 3 Discharge (AE3800A).OM3.Value'\n",
    "\t\t)\n",
    "\t\tAND time BETWEEN TIMESTAMP(¤ start ¤) AND TIMESTAMP(¤ stop ¤)\n",
    "\t\tGROUP BY tagname, time\n",
    "\n",
    "\tUNION ALL\n",
    "\n",
    "-- Mass flow (kg/hr) out of Evap 3 from SCADA\n",
    "\tSELECT\n",
    "\t\ttagname,\n",
    "\t\tTIMESTAMP_TRUNC(time, ¤ granularity ¤) AS time,\n",
    "\t\tAVG(value) AS avg_value\n",
    "\t\tFROM `akbm-houston-prod.houston_data.sensor_data_scada`\n",
    "\t\tWHERE tagname = \n",
    "\t\t\t'FI323113_EvaporatorThreeDischarge'\n",
    "\t\tAND time BETWEEN TIMESTAMP(¤ start ¤) AND TIMESTAMP(¤ stop ¤)\n",
    "\t\tGROUP BY tagname, time\n",
    "\n",
    "\tUNION ALL\n",
    "\n",
    "-- Mass flow (lb/hr) out of Evap 3 from Sulzer2\n",
    "\tSELECT\n",
    "\t\ttagname,\n",
    "\t\tTIMESTAMP_TRUNC(time, ¤ granularity ¤) AS time,\n",
    "\t\tAVG(value) AS avg_value\n",
    "\t\tFROM `akbm-houston-prod.houston_data.sensor_data_sulzer2`\n",
    "\t\tWHERE tagname = \n",
    "\t\t\t'FIT_323113'\n",
    "\t\tAND time BETWEEN TIMESTAMP(¤ start ¤) AND TIMESTAMP(¤ stop ¤)\n",
    "\t\tGROUP BY tagname, time\n",
    "\n",
    "\tUNION ALL\n",
    "\n",
    "-- Viscosity (cP) of evap 3 discharge\n",
    "SELECT\n",
    "\ttagname,\n",
    "\tTIMESTAMP_TRUNC(time, ¤ granularity ¤) AS time,\n",
    "\tAVG(value) AS avg_value\n",
    "\tFROM `akbm-houston-prod.houston_data.sensor_data_sulzer1`\n",
    "\tWHERE tagname IN (\n",
    "\t\t'AE3800B_Viscosity',\t\t\n",
    "\t\t'AE3800B_Viscosity35C'\t\t\n",
    "\t)\n",
    "\tAND time BETWEEN TIMESTAMP(¤ start ¤) AND TIMESTAMP(¤ stop ¤)\n",
    "GROUP BY tagname, time\n",
    "\n",
    "ORDER BY time, tagname;\n",
    "]\\\";\n",
    "\n",
    "sql_str = Eval Insert(sql_template, \"¤\");\n",
    "\n",
    "obj = New SQL Query(\n",
    "  Connection( ODBC ),\n",
    "  Custom SQL( sql_str ),\n",
    "  Query Name( \"Combined Tag Data\" )\n",
    ");\n",
    "dt = obj << Run Foreground();\n",
    "\n",
    "dt2 = dt << Split(\n",
    "  Split By( :tagname ),\n",
    "  Split( :avg_value ),\n",
    "  Group( :time ),\n",
    "  Output Table( \"Combined Split\" ),\n",
    "  Sort by Column Property\n",
    ");\n",
    "Close( dt, no save );\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d44ea-feed-47e4-a4b4-7060e33ad9f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2d9f4-99a5-43fb-9ef2-1be7da07c68c",
   "metadata": {},
   "source": [
    "### The code below is based on the raw data as is in the data warehouse (i.e., 1-second resolution)\n",
    "<u>**NB**</u>: The below code uses resampling with interpolation, which causes issues when production abruptly stops due to a scheduled shutdown, for example. However, comparing the output of the below script to the one using 1-minute resolution, the 12h-integration results are as good as indistinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3379bdfa-518c-4ffb-b231-4d69d2d3f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.integrate import simpson\n",
    "from typing import List, Tuple\n",
    "\n",
    "# ─── USER‐DEFINED PARAMETERS ────────────────────────────────────────────────────\n",
    "START_TIME      = datetime(2025, 5, 10, 11, 0, 0)\n",
    "END_TIME        = datetime(2025, 6, 23, 23, 0, 0)\n",
    "INTERVAL_HOURS  = 12\n",
    "\n",
    "SCADA_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_scada\"\n",
    "TAG_MF_KGHR     = \"FI323113_EvaporatorThreeDischarge\"\n",
    "\n",
    "SULZER_TABLE    = \"akbm-houston-prod.houston_data.sensor_data_sulzer2\"\n",
    "TAG_MF_LBHR     = \"FIT_323113\"\n",
    "\n",
    "FTNIR_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_ftnir\"\n",
    "TAG_PL_PCT      = \"Evaporator 3 Discharge (AE3800A).Phospholipids.Value\"\n",
    "\n",
    "OUT_CSV         = \"evap3_integration_summary.csv\"\n",
    "\n",
    "# ─── 1) CONNECT TO BigQuery via ODBC ──────────────────────────────────────────\n",
    "dsn = 'bq64_system'\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)\n",
    "\n",
    "# ─── 2) FUNCTION TO PULL RAW SERIES AND RESAMPLE TO 1-SECOND ─────────────────\n",
    "def fetch_raw_series(\n",
    "    conn: pyodbc.Connection,\n",
    "    dataset_table: str,\n",
    "    tagname: str,\n",
    "    start_time: datetime,\n",
    "    end_time: datetime\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls raw timestamp, value for the given tag and period,\n",
    "    resamples to true 1-second frequency with linear interpolation.\n",
    "    \"\"\"\n",
    "    t0 = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    t1 = end_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "      time AS timestamp,\n",
    "      value\n",
    "    FROM `{dataset_table}`\n",
    "    WHERE tagname = '{tagname}'\n",
    "      AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "    ORDER BY timestamp\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql(sql, conn)\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "    df = df.set_index(\"timestamp\").sort_index()\n",
    "\n",
    "    # Resample to 1-second frequency, interpolate missing points\n",
    "    df = df.resample('1S').mean().interpolate()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ─── 3) PULL THE THREE SERIES AND MERGE THEM ─────────────────────────────────\n",
    "df_kg  = fetch_raw_series(conn, SCADA_TABLE, TAG_MF_KGHR, START_TIME, END_TIME)\n",
    "df_lb  = fetch_raw_series(conn, SULZER_TABLE, TAG_MF_LBHR, START_TIME, END_TIME)\n",
    "df_plp = fetch_raw_series(conn, FTNIR_TABLE, TAG_PL_PCT,  START_TIME, END_TIME)\n",
    "\n",
    "df_kg  = df_kg.rename(columns={\"value\": \"mass_flow_kg_hr\"})\n",
    "df_lb  = df_lb.rename(columns={\"value\": \"mass_flow_lb_hr\"})\n",
    "df_plp = df_plp.rename(columns={\"value\": \"pl_pct\"})\n",
    "\n",
    "df_all = df_kg.join(df_lb, how=\"inner\").join(df_plp, how=\"inner\")\n",
    "\n",
    "# ─── 4) COMPUTE “TOTAL PL MASS” COLUMNS ──────────────────────────────────────\n",
    "df_all[\"pl_mass_kg_hr\"] = df_all[\"mass_flow_kg_hr\"] * (df_all[\"pl_pct\"] / 100.0)\n",
    "df_all[\"pl_mass_lb_hr\"] = df_all[\"mass_flow_lb_hr\"] * (df_all[\"pl_pct\"] / 100.0)\n",
    "\n",
    "# ─── 5) BUILD INTEGRATION INTERVALS ──────────────────────────────────────────\n",
    "def build_intervals_from_start(\n",
    "    start: datetime,\n",
    "    end: datetime,\n",
    "    interval_hours: int\n",
    ") -> List[Tuple[datetime, datetime]]:\n",
    "    intervals: List[Tuple[datetime, datetime]] = []\n",
    "    cursor = start\n",
    "    delta = timedelta(hours=interval_hours)\n",
    "\n",
    "    while cursor < end:\n",
    "        next_point = cursor + delta\n",
    "        intervals.append((cursor, min(next_point, end)))\n",
    "        cursor = next_point\n",
    "\n",
    "    return intervals\n",
    "\n",
    "interval_list = build_intervals_from_start(START_TIME, END_TIME, INTERVAL_HOURS)\n",
    "\n",
    "# ─── 6) INTEGRATION FUNCTION REMAINS UNCHANGED ──────────────────────────────\n",
    "def integrate_interval(\n",
    "    df: pd.DataFrame,\n",
    "    col: str,\n",
    "    start: datetime,\n",
    "    end: datetime\n",
    ") -> Tuple[float, float, float, float]:\n",
    "    df_int = df.loc[start : end - timedelta(seconds=1), col].dropna()\n",
    "    if df_int.empty:\n",
    "        return (0.0, 0.0, 0.0, 0.0)\n",
    "\n",
    "    t_secs = (df_int.index.view(np.int64) // 10**9).astype(float)\n",
    "    t_hours = t_secs / 3600.0\n",
    "    y = df_int.values\n",
    "\n",
    "    trapz_val = np.trapz(y, x=t_hours)\n",
    "\n",
    "    if len(y) >= 3:\n",
    "        if len(y) % 2 == 0:\n",
    "            simpson_val = simpson(y[:-1], x=t_hours[:-1])\n",
    "        else:\n",
    "            simpson_val = simpson(y, x=t_hours)\n",
    "    else:\n",
    "        simpson_val = np.nan\n",
    "\n",
    "    if len(y) >= 2:\n",
    "        riemann_val = np.sum(y[:-1] * (t_hours[1:] - t_hours[:-1]))\n",
    "    else:\n",
    "        riemann_val = 0.0\n",
    "\n",
    "    agg_sum_val = np.sum(y) * (1.0 / 3600.0)\n",
    "\n",
    "    return (trapz_val, simpson_val, riemann_val, agg_sum_val)\n",
    "\n",
    "# ─── 7) LOOP OVER INTERVALS AND BUILD SUMMARY TABLE ─────────────────────────\n",
    "records = []\n",
    "for (t0, t1) in interval_list:\n",
    "    trapz_kg, simps_kg, riem_kg, agg_kg         = integrate_interval(df_all, \"mass_flow_kg_hr\", t0, t1)\n",
    "    trapz_plkg, simps_plkg, riem_plkg, agg_plkg = integrate_interval(df_all, \"pl_mass_kg_hr\",   t0, t1)\n",
    "    trapz_lb, simps_lb, riem_lb, agg_lb         = integrate_interval(df_all, \"mass_flow_lb_hr\", t0, t1)\n",
    "    trapz_pllb, simps_pllb, riem_pllb, agg_pllb = integrate_interval(df_all, \"pl_mass_lb_hr\",   t0, t1)\n",
    "\n",
    "    records.append({\n",
    "        \"interval_start\":    t0,\n",
    "        \"interval_end\":      t1,\n",
    "        \"trapz_mass_kg\":     trapz_kg,\n",
    "        \"simpson_mass_kg\":   simps_kg,\n",
    "        \"riemann_mass_kg\":   riem_kg,\n",
    "        \"agg_sum_mass_kg\":   agg_kg,\n",
    "        \"trapz_pl_kg\":       trapz_plkg,\n",
    "        \"simpson_pl_kg\":     simps_plkg,\n",
    "        \"riemann_pl_kg\":     riem_plkg,\n",
    "        \"agg_sum_pl_kg\":     agg_plkg,\n",
    "        \"trapz_mass_lb\":     trapz_lb,\n",
    "        \"simpson_mass_lb\":   simps_lb,\n",
    "        \"riemann_mass_lb\":   riem_lb,\n",
    "        \"agg_sum_mass_lb\":   agg_lb,\n",
    "        \"trapz_pl_lb\":       trapz_pllb,\n",
    "        \"simpson_pl_lb\":     simps_pllb,\n",
    "        \"riemann_pl_lb\":     riem_pllb,\n",
    "        \"agg_sum_pl_lb\":     agg_pllb,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame.from_records(records)\n",
    "summary_df[\"interval_start\"] = pd.to_datetime(summary_df[\"interval_start\"])\n",
    "summary_df[\"interval_end\"]   = pd.to_datetime(summary_df[\"interval_end\"])\n",
    "summary_df = summary_df.sort_values(\"interval_start\").reset_index(drop=True)\n",
    "\n",
    "# ─── 8) WRITE SUMMARY TABLE TO CSV ───────────────────────────────────────────\n",
    "summary_df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Wrote summary table with {len(summary_df)} intervals to → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dddf7e-2b71-464b-a7c6-8a76c2455287",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89f07b-be6e-458e-9ab2-2e3f0855a17d",
   "metadata": {},
   "source": [
    "### Not sure what the below was written for, but looks like it can be used for raw data investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6c3f4-b68e-4c8f-a88b-b0c1f0e7f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used for analytical purpose to get the 1'' raw data from the dwh\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "# ─── USER‐DEFINED PARAMETERS ────────────────────────────────────────────────────\n",
    "# 1) Time window for data pull (UTC)\n",
    "START_TIME   = datetime(2025, 6, 11, 23, 0, 0)   # yyyy, m, d, H, M, S\n",
    "STOP_TIME    = datetime(2025, 6, 12, 11, 0, 0) # yyyy, m, d, H, M, S\n",
    "\n",
    "# 2) Granularity for TIMESTAMP_TRUNC (e.g. \"SECOND\", \"MINUTE\", \"HOUR\")\n",
    "GRANULARITY = \"SECOND\"\n",
    "\n",
    "# 3) BigQuery tables & tag lists\n",
    "FTNIR_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_ftnir\"\n",
    "FTNIR_TAGS      = [\n",
    "    \"Evaporator 3 Discharge (AE3800A).Phospholipids.Value\",\n",
    "    \"Evaporator 3 Discharge (AE3800A).Ethanol.Value\",\n",
    "    \"Evaporator 3 Discharge (AE3800A).OM3.Value\"\n",
    "]\n",
    "\n",
    "SCADA_TABLE     = \"akbm-houston-prod.houston_data.sensor_data_scada\"\n",
    "SCADA_TAG       = \"FI323113_EvaporatorThreeDischarge\"\n",
    "\n",
    "SULZER2_TABLE   = \"akbm-houston-prod.houston_data.sensor_data_sulzer2\"\n",
    "SULZER2_TAG     = \"FIT_323113\"\n",
    "\n",
    "SULZER1_TABLE   = \"akbm-houston-prod.houston_data.sensor_data_sulzer1\"\n",
    "SULZER1_TAGS    = [\n",
    "    \"AE3800B_Viscosity\",\n",
    "    \"AE3800B_Viscosity35C\"\n",
    "]\n",
    "\n",
    "# 4) Output CSV filename\n",
    "OUT_CSV = \"evap3_one_second_sample.csv\"\n",
    "\n",
    "# ─── 1) CONNECT TO BigQuery via ODBC ──────────────────────────────────────────\n",
    "dsn = 'bq64_system'\n",
    "conn = pyodbc.connect(f\"DSN={dsn}\", autocommit=True)\n",
    "\n",
    "# ─── 2) BUILD AND RUN UNION ALL SQL FOR MULTIPLE TAGS ─────────────────────────\n",
    "def fetch_all_tags_1sec(\n",
    "    conn: pyodbc.Connection,\n",
    "    start_time: datetime,\n",
    "    stop_time: datetime,\n",
    "    granularity: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Pulls 1-second truncated, per-second averages for all relevant tags across\n",
    "    FTNIR, SCADA, Sulzer2, and Sulzer1 tables. Returns a DataFrame with columns:\n",
    "        ┌────────────┬──────────────────────────────────────────┬──────────┐\n",
    "        │   time     │                tagname                  │  value   │\n",
    "        ├────────────┼──────────────────────────────────────────┼──────────┤\n",
    "        │ 2025-05-10 00:00:00 │ Evaporator 3 … Phospholipids.Value   │  12.345  │\n",
    "        │ 2025-05-10 00:00:00 │ Evaporator 3 … Ethanol.Value        │   0.123  │\n",
    "        │       …    │                    …                     │    …     │\n",
    "        └────────────┴──────────────────────────────────────────┴──────────┘\n",
    "    \"\"\"\n",
    "\n",
    "    t0 = start_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    t1 = stop_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Build the UNION ALL SQL string\n",
    "    sql_parts: List[str] = []\n",
    "\n",
    "    # 2.1) FTNIR tags\n",
    "    ft_columns = \",\\n    \".join(f\"'{tag}'\" for tag in FTNIR_TAGS)\n",
    "    sql_ftnir = f\"\"\"\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(time, {granularity}) AS time,\n",
    "  tagname,\n",
    "  AVG(value) AS value\n",
    "FROM `{FTNIR_TABLE}`\n",
    "WHERE tagname IN ({ft_columns})\n",
    "  AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "GROUP BY tagname, time\n",
    "\"\"\"\n",
    "    sql_parts.append(sql_ftnir)\n",
    "\n",
    "    # 2.2) SCADA tag\n",
    "    sql_scada = f\"\"\"\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(time, {granularity}) AS time,\n",
    "  tagname,\n",
    "  AVG(value) AS value\n",
    "FROM `{SCADA_TABLE}`\n",
    "WHERE tagname = '{SCADA_TAG}'\n",
    "  AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "GROUP BY tagname, time\n",
    "\"\"\"\n",
    "    sql_parts.append(sql_scada)\n",
    "\n",
    "    # 2.3) Sulzer2 tag\n",
    "    sql_sulzer2 = f\"\"\"\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(time, {granularity}) AS time,\n",
    "  tagname,\n",
    "  AVG(value) AS value\n",
    "FROM `{SULZER2_TABLE}`\n",
    "WHERE tagname = '{SULZER2_TAG}'\n",
    "  AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "GROUP BY tagname, time\n",
    "\"\"\"\n",
    "    sql_parts.append(sql_sulzer2)\n",
    "\n",
    "    # 2.4) Sulzer1 tags\n",
    "    s1_columns = \",\\n    \".join(f\"'{tag}'\" for tag in SULZER1_TAGS)\n",
    "    sql_sulzer1 = f\"\"\"\n",
    "SELECT\n",
    "  TIMESTAMP_TRUNC(time, {granularity}) AS time,\n",
    "  tagname,\n",
    "  AVG(value) AS value\n",
    "FROM `{SULZER1_TABLE}`\n",
    "WHERE tagname IN ({s1_columns})\n",
    "  AND time BETWEEN TIMESTAMP('{t0}') AND TIMESTAMP('{t1}')\n",
    "GROUP BY tagname, time\n",
    "\"\"\"\n",
    "    sql_parts.append(sql_sulzer1)\n",
    "\n",
    "    # Combine all parts with UNION ALL and final ORDER BY\n",
    "    union_sql = \"\\nUNION ALL\\n\".join(sql_parts)\n",
    "    full_sql = f\"\"\"\n",
    "{union_sql}\n",
    "ORDER BY time, tagname\n",
    "\"\"\"\n",
    "\n",
    "    # Execute and return DataFrame\n",
    "    df = pd.read_sql(full_sql, conn)\n",
    "    # Ensure timestamp column is parsed as pandas datetime\n",
    "    df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "    return df\n",
    "\n",
    "# Fetch raw per-second averages for all tags\n",
    "df_raw = fetch_all_tags_1sec(conn, START_TIME, STOP_TIME, GRANULARITY)\n",
    "\n",
    "# ─── 3) PIVOT RAW DATA SO EACH TAG BECOMES A COLUMN ───────────────────────────\n",
    "# Resulting DataFrame will have index = time, columns = tagname, values = avg_value\n",
    "df_pivot = df_raw.pivot(index=\"time\", columns=\"tagname\", values=\"value\")\n",
    "\n",
    "# Optionally reorder columns in a sensible order:\n",
    "desired_order = (\n",
    "    FTNIR_TAGS\n",
    "    + [SCADA_TAG]\n",
    "    + [SULZER2_TAG]\n",
    "    + SULZER1_TAGS\n",
    ")\n",
    "df_pivot = df_pivot.reindex(columns=desired_order)\n",
    "\n",
    "# ─── 4) WRITE TO CSV ─────────────────────────────────────────────────────────\n",
    "df_pivot.to_csv(OUT_CSV, index_label=\"time\")\n",
    "print(f\"Wrote {len(df_pivot)} rows × {len(df_pivot.columns)} tags → {OUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
